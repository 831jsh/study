## 1강 개요

- explicit programming -> too many rules (개발하기 어려움)
- 1959년 Arthur가 현상을 통해 배우는, 데이터를 학습하는 머신러닝 개념을 만듬

### Supervised/Unsupervised learning

#### Supervised learning

- learning with labled examples
- 고양이/개 사진 분류
- 데이터를 카테고리해서 학습함


#### Unsupervised learning

- un-labeled data
- 구글 뉴스/단어 클러스터링
- 데이터를 보고 스스로 학습함


#### Training data set

- Y : 값
- X : feature
- feature를 가지고 labeled된 Y를 학습함


#### Types of supervised learning

- 시험 점수 예측 -> regression
- 시험 패스/논패스 예측 -> binary classification
- 학점 예측 -> multi-label classification



### TensorFlow

- data flow 그래프를 사용해서 numerical한 계산을 함
- 파이썬 기반


#### Data Flow Graph

- node가 edge로 연결되어 있음
- 돌아다니는 데이터 -> tensor
- tensor가 돌아다닌다 -> tensor flow

#### Tensor flow 설치

#### Tensor flow 예제 코드

- https://github.com/hunkim/DeepLearningZeroToAll

```
hello = tf.constatnt("Hello") -> node를 생성
sess - tf.Session() -> computational graph를 생성하기 위해 세션을 생성
print(sess.run(hello)) -> node 실행
```


```
node1 = tf.constant(3.0, tf.float32)
node2 = tf.constant(4.0)
node3 = tf.add(node1, node2)

// 그대로 출력하면 Tensor 정보가 나옴
print("node1:", node1)

sess = tf.Session()
print("sess.run(node3)", sess.run(node3))
```

#### Placeholder

```
a = tf.placeholder(tf.float32)
b = tf.placeholder(tf.float32)
adder_node = a + b

print(sess.run(adder_node, feed_dict=:a: 3, b: 4.5}))
print(sess.run(adder_node, feed_dict=:a: [1, 3], b: [2, 4]}))
```

#### TensorFlow Mechanics

- graph를 빌드
- 세션 생성 후 실행 (값이 이때 입력됨)
- 결과


#### Tensor

- rank, shapes, types

- rank (몇 차원 배열인지)
  - scalar(magnitude) = 483 -> rank 0
  - vector(magnitude + direction) = [1.1, 2.2] -> rank 1
  - Matrix(table of numbers) = [[1.1, 2.2]. [4.2, 5.5]] -> rank 2
  - 3-Tensor(cube of numbers) = [[[2], [4], [6]], [8], [10], [12]], [14], [16], [18]]] -> rank 3
  - n-Tensor(you get idea)
  
- shapes (element가 몇 개인지)
  - 0 rank -> [] shape -> 0-D tensor
  - 1 rank -> [D0] shape -> 1-D tensor
  - 2 rank -> [D0, D1] shape -> 2-D tensor
  - 3 rank -> [D0, D1, D2] shape -> 3-D tensor
  - n rank -> [D0... Dn-1] shape -> n-D tensor
  - 한 마디로 rank의 형태에 대해서 설명해주는 것, 2차원 배열을 shape으로 표현하면 [[배열차원 수(가장 안쪽부터)],[element 수]]
  - 가장 안 쪽요소부터 설명해주는 것
  - 4 rank이면 가장 안 쪽의 element 수가 제일 끝에, 그 다음 최소의 element
  - 그러니까 rank의 수 만큼 shape의 element 종류 수가 나옴
  
  
  [![shape1](https://github.com/leeplay/study/blob/master/machine-learning/image/shape1.png)]()

- axis
  - 축인데 shape 을 보고 가장 바깥쪽이 0 이고 안으로 들어갈 수록(배열의 차원이 바뀔수록) 1씩 증가함
  - -1을 가장 안쪽 차원으로 표현하기도 함
  - reduce/argmax 계산할 때 사용함

- reshape 
  - 자료의 차원을 재조정하는 것
  - 보통 element 수는 유지해줌
  - squeeze, expand_dims 도 지원함

- one_hot
  - 자리 수 만큼 배열로 표현해 유효한 부분에 1로 표현해줌
  - depth만큼 자릿수로 표현해 줌

- Types
  - DT_FLOAT -> tf.float32
  - DT_DOUBLE -> tf.float64
  - DT_INT8 -> tf.int8
  - DT_INT16 -> tf.int16
  - DT_INT32-> tf.int32
  - DT_INT64 -> tf.int64
